{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT57z3Kq6E7z",
        "outputId": "ebf69f8f-7517-4422-d320-e98318623605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsa87LLn6aAw",
        "outputId": "8a905627-795c-4c8a-ac50-1e6c2a3e671c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 57.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=90ebdb9a825cb2832e683614f306d2a951f4cb9da4769f5fed3c3e6bf332ede3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soV3Z14B7CKz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import Parameter\n",
        "from transformers import BertModel\n",
        "from transformers import BertConfig\n",
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertLayer\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "dtb3XhKv849I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8sMHFHp7FcS"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE_-QaN0DX6X"
      },
      "outputs": [],
      "source": [
        "def get_df(file):\n",
        "    return pd.read_csv(file, sep='|',header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QECKknXvD9CD"
      },
      "outputs": [],
      "source": [
        "train_df_fake = get_df('/content/drive/My Drive/WeiboRumorSet/WeiboRumorSet/tweets/train_rumor.txt')\n",
        "train_df_real = get_df('/content/drive/My Drive/WeiboRumorSet/WeiboRumorSet/tweets/train_nonrumor.txt')\n",
        "test_df_fake = get_df('/content/drive/My Drive/WeiboRumorSet/WeiboRumorSet/tweets/test_rumor.txt')\n",
        "test_df_real = get_df('/content/drive/My Drive/WeiboRumorSet/WeiboRumorSet/tweets/test_nonrumor.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeesQVhmEN-G"
      },
      "outputs": [],
      "source": [
        "train_fake = train_df_fake[0].tolist()\n",
        "train_real = train_df_real[0].tolist()\n",
        "test_fake = test_df_fake[0].tolist()\n",
        "test_real = test_df_real[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXSZwwNFAPwZ"
      },
      "outputs": [],
      "source": [
        "def fix_offset(list_):\n",
        "    fixed_flag = False\n",
        "\n",
        "    while not fixed_flag:\n",
        "        exit_flag=False\n",
        "        temp = copy.deepcopy(list_)\n",
        "        for i,v in enumerate(temp):\n",
        "            if v!=None:\n",
        "                if 'sinaimg.cn' in v:\n",
        "                    if list_[i+1] !=None:\n",
        "                        if list_[i+1].isdigit():\n",
        "                            list_.insert(i+1,None)\n",
        "                            exit_flag=True\n",
        "                            break\n",
        "        if not exit_flag:\n",
        "            fixed_flag=True\n",
        "            \n",
        "    return list_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVcK9wT4ASoB"
      },
      "outputs": [],
      "source": [
        "train_fake = fix_offset(train_fake) \n",
        "train_real = fix_offset(train_real)\n",
        "test_fake = fix_offset(test_fake)\n",
        "test_real = fix_offset(test_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MNsZVuuAYBX"
      },
      "outputs": [],
      "source": [
        "def break_in_block(list_):\n",
        "    temp = []\n",
        "    for i in range(0,len(list_),3):\n",
        "        temp.append(list_[i:i+3])\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adld7d9xAZKg"
      },
      "outputs": [],
      "source": [
        "train_fake = break_in_block(train_fake)\n",
        "train_real = break_in_block(train_real)\n",
        "test_fake = break_in_block(test_fake)\n",
        "test_real = break_in_block(test_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7xAnDugAiMw"
      },
      "outputs": [],
      "source": [
        "def get_image_and_text_list(blocks_list):\n",
        "    image_list = []\n",
        "    text_list = []\n",
        "    for i in blocks_list:\n",
        "        if i[-1] !=None:\n",
        "            image_list.append(i[1])\n",
        "            text_list.append(i[-1])\n",
        "    image_list = [i.split('/')[-1] for i in image_list]\n",
        "    return image_list, text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSqmQPhtAk30"
      },
      "outputs": [],
      "source": [
        "train_fake_image,train_fake_text = get_image_and_text_list(train_fake)\n",
        "train_real_image,train_real_text = get_image_and_text_list(train_real)\n",
        "test_fake_image,test_fake_text = get_image_and_text_list(test_fake)\n",
        "test_real_image,test_real_text = get_image_and_text_list(test_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTCSD2yAoCS"
      },
      "outputs": [],
      "source": [
        "train_fake_Y = [0]*len(train_fake_image)\n",
        "train_real_Y = [1]*len(train_real_image)\n",
        "test_fake_Y = [0]*len(test_fake_image)\n",
        "test_real_Y = [1]*len(test_real_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMolhYaBAr2X"
      },
      "outputs": [],
      "source": [
        "train_images = train_fake_image+train_real_image\n",
        "train_text = train_fake_text + train_real_text\n",
        "trainY = train_fake_Y+train_real_Y\n",
        "\n",
        "test_images = test_fake_image+test_real_image\n",
        "test_text = test_fake_text+test_real_text\n",
        "testY = test_fake_Y+test_real_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWZ31XxGAvCh",
        "outputId": "e239ad16-eedd-40f4-de2a-0474f0fef5b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7481, 7481, 7481, 1930, 1930, 1930)"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_images),len(train_text),len(trainY),len(test_images),len(test_text),len(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUKFw8V9AzAc"
      },
      "outputs": [],
      "source": [
        "train_images = np.array(train_images)\n",
        "train_text = np.array(train_text)\n",
        "trainY = np.array(trainY)\n",
        "test_images = np.array(test_images)\n",
        "test_text = np.array(test_text)\n",
        "testY = np.array(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbFEYcqFuYv6"
      },
      "outputs": [],
      "source": [
        "from os import walk\n",
        "def get_total_images() :\n",
        "  mypath = \"/content/drive/My Drive/multi-modal/weibo/airsplay/feat_rumor/\"\n",
        "  mypath1 = \"/content/drive/My Drive/multi-modal/weibo/airsplay/feat_nonrumor/\"\n",
        "  img_feat = []\n",
        "  for (dirpath, dirnames, filenames) in walk(mypath):\n",
        "      img_feat.extend(filenames)\n",
        "      break\n",
        "  for (dirpath, dirnames, filenames) in walk(mypath1):\n",
        "      img_feat.extend(filenames)\n",
        "      break \n",
        "  mypath3 = \"/content/drive/My Drive/WeiboRumorSet/WeiboRumorSet/rumor_images/\"\n",
        "  images = []\n",
        "  for (dirpath, dirnames, filenames) in walk(mypath3):\n",
        "      images.extend(filenames)\n",
        "      break\n",
        "  mypath4 = \"/content/drive/My Drive/WeiboRumorSet/WeiboRumorSet/nonrumor_images/\"\n",
        "  for (dirpath, dirnames, filenames) in walk(mypath4):\n",
        "      images.extend(filenames)\n",
        "      break\n",
        "  img_dict = {}\n",
        "  for img in images :\n",
        "    if (img[:-4] not in img_dict) :\n",
        "      img_dict[img[:-4]] = img\n",
        "    elif ((img[:-4] in img_dict) and img[-4:]=='.gif') :\n",
        "      img_dict[img[:-4]] = img \n",
        "  total_img = []\n",
        "  for img in img_feat :\n",
        "    total_img.append(img_dict[img[:-4]])\n",
        "  return total_img, images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB3MPIbTwwGr",
        "outputId": "1e1c62ff-d2f8-4692-b9ba-dcd98185b960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read feat rumor\n",
            "Read feat nonrumor\n",
            "Read feat rumor\n",
            "Read feat non rumor\n"
          ]
        }
      ],
      "source": [
        "total_images, images = get_total_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXhrocIuA325"
      },
      "outputs": [],
      "source": [
        "def index_to_delete(list_, total_images):\n",
        "    gif_list = ['957e1cf2tw1e5foxts295g206o03p4qp.gif','a716fd45jw1ev0cgf8j46g209505zh4i.gif','005vnhZYgw1evupo8ttddg308w06o4qp.gif','7da75521gw1ele2jvi85rg2096056u0x.gif']\n",
        "    index = []\n",
        "    for i,v in enumerate(list_):\n",
        "        if v[:-4] not in total_images:\n",
        "            index.append(i)\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKBr_zCQA6sR",
        "outputId": "9a8182c5-4ef9-4781-dd96-8a5136c2c87b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4143"
            ]
          },
          "execution_count": 45,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_delete_index =index_to_delete(train_images, t_img)\n",
        "test_delete_index = index_to_delete(test_images, t_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrrLkVhU1rUP"
      },
      "outputs": [],
      "source": [
        "train_images = np.delete(train_images,train_delete_index)\n",
        "train_text = np.delete(train_text,train_delete_index)\n",
        "trainY = np.delete(trainY,train_delete_index)\n",
        "test_images = np.delete(test_images,test_delete_index)\n",
        "test_text = np.delete(test_text,test_delete_index)\n",
        "testY = np.delete(testY,test_delete_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ-nCMzMA9q_"
      },
      "outputs": [],
      "source": [
        "shuffle_index= np.arange(len(train_images))\n",
        "np.random.shuffle(shuffle_index)\n",
        "train_images = train_images[shuffle_index]\n",
        "train_text = train_text[shuffle_index]\n",
        "trainY = trainY[shuffle_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D2Xp4G1CD2h",
        "outputId": "6d3c25c7-7c5e-4e74-f92a-d77dae69107e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4142, 4142, 4142, 1126, 1126, 1126)"
            ]
          },
          "execution_count": 50,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_images),len(train_text),len(trainY),len(test_images),len(test_text),len(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOItB7SNNfvO"
      },
      "outputs": [],
      "source": [
        "# Load feature file which is a dictionary with image_id as key and feature vector as value. The feature vector is created using bottom up attention.\n",
        "dict_feat = pickle.load(open(\"/content/drive/My Drive/multi-modal/weibo/airsplay/dict_feat.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nOXTCrxNgx_",
        "outputId": "7a88d645-f85b-4163-e575-74288909d837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "677\n",
            "677\n"
          ]
        }
      ],
      "source": [
        "# Find images for which feature vector size has a mismatch (the total number of fragments are not 36).\n",
        "c=0\n",
        "shape_not_match = []\n",
        "for key in dict_feat :\n",
        "  if(dict_feat[key].shape[0]!=36) :\n",
        "    shape_not_match.append(key)\n",
        "    c+=1\n",
        "print(c)\n",
        "print(len(shape_not_match))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uqTN0eCNRJ_"
      },
      "outputs": [],
      "source": [
        "def index_to_delete_shape(images, shape_not_match) :\n",
        "  index = []\n",
        "  for i in range(len(images)) :\n",
        "    main_image = images[i]\n",
        "    if ((main_image[:-4] in shape_not_match)) :\n",
        "      index.append(i)\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Ha1kysNrwq",
        "outputId": "f10a3422-efb4-4b94-bf22-3d80f48ad586"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "677"
            ]
          },
          "execution_count": 58,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_delete_index =index_to_delete_shape(train_images, shape_not_match)\n",
        "test_delete_index = index_to_delete_shape(test_images, shape_not_match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meUZEMgKN1hF"
      },
      "outputs": [],
      "source": [
        "# Keep only those images for which the feature vector has 36 fragments \n",
        "train_images1 = np.delete(train_images,train_delete_index)\n",
        "train_text1 = np.delete(train_text,train_delete_index)\n",
        "trainY1 = np.delete(trainY,train_delete_index)\n",
        "test_images1 = np.delete(test_images,test_delete_index)\n",
        "test_text1 = np.delete(test_text,test_delete_index)\n",
        "testY1 = np.delete(testY,test_delete_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNmM4FJq3XSN",
        "outputId": "5b276700-942e-43bc-ed46-99062d1b3738"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3618, 3618, 3618, 973, 973, 973)"
            ]
          },
          "execution_count": 60,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_images1),len(train_text1),len(trainY1),len(test_images1),len(test_text1),len(testY1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccHkX0gn7sGT"
      },
      "outputs": [],
      "source": [
        "train_input_img = torch.zeros((3618,36,2048))\n",
        "train_output_img = torch.zeros((3618,2))\n",
        "\n",
        "for i in range(len(train_images1)) :\n",
        "  main_image = train_images1[i]\n",
        "  train_input_img[i] = dict_feat[main_image[:-4]]\n",
        "  if (trainY1[i]==0) :\n",
        "    train_output_img[i][0] = 1\n",
        "  else :\n",
        "    train_output_img[i][1] = 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxlEkX5N8HkF"
      },
      "outputs": [],
      "source": [
        "test_input_img = torch.zeros((973,36,2048))\n",
        "test_output_img = torch.zeros((973,2))\n",
        "\n",
        "for i in range(len(test_images1)) :\n",
        "  main_image = test_images1[i]\n",
        "  test_input_img[i] = dict_feat[main_image[:-4]]\n",
        "  if (testY1[i]==0) :\n",
        "    test_output_img[i][0] = 1\n",
        "  else :\n",
        "    test_output_img[i][1] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTj6eFRi8KcY",
        "outputId": "316c9891-1932-460c-90d5-046c9e5d2089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3618\n",
            "3618\n"
          ]
        }
      ],
      "source": [
        "print(len(train_input_img))\n",
        "print(len(train_output_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Model"
      ],
      "metadata": {
        "id": "fzy_IU5HAKIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YH9hZxp4oq7"
      },
      "outputs": [],
      "source": [
        "# Code for this cell is from https://github.com/yiling2018/saem/blob/master/bert.py\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class BERTLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BERTLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "class BERTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BERTSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTAttention, self).__init__()\n",
        "        self.self = BERTSelfAttention(config)\n",
        "        self.output = BERTSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BERTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = gelu\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTLayer, self).__init__()\n",
        "        self.attention = BERTAttention(config)\n",
        "        self.intermediate = BERTIntermediate(config)\n",
        "        self.output = BERTOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8onzhCH36VSx"
      },
      "outputs": [],
      "source": [
        "class TransformerMapping(nn.Module):\n",
        "    \"\"\" Self-attention layer for image branch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerMapping, self).__init__()\n",
        "        bert_config = BertConfig.from_json_file(\"t_cfg_weibo.json\")\n",
        "        self.layer = BERTLayer(bert_config)\n",
        "        self.mapping = nn.Linear(2048, 128)\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.cls_layer = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mapping(x) # x: (batch_size, patch_num, final_dims)\n",
        "        attention_mask = torch.ones(x.size(0), x.size(1))\n",
        "        if torch.cuda.is_available():\n",
        "            attention_mask = attention_mask.cuda()\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.float()\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        hidden_states = self.layer(x, extended_attention_mask)\n",
        "        embed = torch.mean(hidden_states, 1) # (batch_size, final_dims)\n",
        "        codes = F.normalize(embed, p=2, dim=1)  # (N, C)\n",
        "        codes = self.dropout1(codes)\n",
        "        codes = self.cls_layer(codes)\n",
        "        codes = F.softmax(codes, dim=1)\n",
        "        return codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxC7f6ILgjdz"
      },
      "source": [
        "## Text embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ickNx4HjgnPr"
      },
      "outputs": [],
      "source": [
        "def freeze_layers(model):\n",
        "    for child in model.children():\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMN-zGd8hIN-"
      },
      "outputs": [],
      "source": [
        "class BertMapping(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BertMapping, self).__init__()\n",
        "        bert_config = BertConfig.from_pretrained('bert-base-chinese')\n",
        "        self.bert = BertModel(bert_config)\n",
        "        freeze_layers(self.bert)\n",
        "        final_dims = 128\n",
        "        Ks = [1, 2, 3]\n",
        "        in_channel = 1\n",
        "        out_channel = 512\n",
        "        embedding_dim = bert_config.hidden_size\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(in_channel, out_channel, (K, embedding_dim)) for K in Ks])\n",
        "        self.dropout = nn.Dropout(bert_config.hidden_dropout_prob)\n",
        "        self.mapping = nn.Linear(len(Ks)*out_channel, final_dims)\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.cls_layer = nn.Linear(final_dims, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids,attention_mask=attention_mask, return_dict=True)\n",
        "        x = outputs.last_hidden_state.unsqueeze(1)  # (batch_size, 1, token_num, embedding_dim)\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(batch_size, out_channel, W), ...]*len(Ks)\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "        output = torch.cat(x, 1)\n",
        "\n",
        "        output = self.dropout(output)\n",
        "        code = self.mapping(output)\n",
        "        # code = F.tanh(code)\n",
        "        code = F.normalize(code, p=2, dim=1)\n",
        "        code = self.dropout1(code)\n",
        "        code = self.cls_layer(code)\n",
        "        code = F.softmax(code, dim=1)\n",
        "        return code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "76aa08b2f663499d834a967000cb705d",
            "603ff023842f46faa08d96376f5e8aee",
            "0d768311b21e4ef5833f4df1199974be",
            "48aa0edd2ebd4cf3a4b624db71d958c3",
            "f310c7ca12a94e7fb60fb144e77566bc",
            "ba0f496bbda14457bfb1b6883874fdb8",
            "849fcde145e5449c84c90d085dbb0a53",
            "6deada4511504449aa3b3f616ea976ad"
          ]
        },
        "id": "FYqh0-vMIJbl",
        "outputId": "7e09df87-420d-4375-da02-d492816e5555"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76aa08b2f663499d834a967000cb705d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVYlnuFZIi68"
      },
      "outputs": [],
      "source": [
        "def get_token_ids(x_train, x_test):\n",
        "    \n",
        "    token_tr = []\n",
        "    token_tst = []\n",
        "    count = 0\n",
        "    for sent in x_train :\n",
        "        tokens = tokenizer.encode(sent, add_special_tokens = True, max_length=512)\n",
        "        token_tr.append(tokens)\n",
        "        count+=1\n",
        "        if(count%1000==0):\n",
        "            print(count)\n",
        "    \n",
        "    for sent1 in x_test :\n",
        "        tokens1 = tokenizer.encode(sent1, add_special_tokens = True, max_length=512)\n",
        "        token_tst.append(tokens1)\n",
        "        count+=1\n",
        "        if(count%1000==0):\n",
        "            print(count)\n",
        "            \n",
        "    return token_tr, token_tst "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUSgvPHsIYly",
        "outputId": "216c6e81-ac95-4301-bc0e-facb6aea026c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n"
          ]
        }
      ],
      "source": [
        "xtr_token, xtst_token = get_token_ids(train_text1, test_text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxiWTl7tIbY_"
      },
      "outputs": [],
      "source": [
        "xtr_token = pad_sequences(xtr_token, maxlen=512, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "xtst_token = pad_sequences(xtst_token, maxlen=512, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aHomHaRIcVN"
      },
      "outputs": [],
      "source": [
        "attention_mask_tr = []\n",
        "attention_mask_tst = []\n",
        "for sent in xtr_token:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_mask_tr.append(att_mask)\n",
        "\n",
        "for sent in xtst_token:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_mask_tst.append(att_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRy6BroQIOD3"
      },
      "outputs": [],
      "source": [
        "train_input_text = torch.tensor(xtr_token)\n",
        "test_input_text = torch.tensor(xtst_token)\n",
        "train_mask = torch.tensor(attention_mask_tr)\n",
        "test_mask = torch.tensor(attention_mask_tst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHM43U7N5h3h"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7AooyYZ5uIF"
      },
      "outputs": [],
      "source": [
        "class FinalModel(nn.Module) :\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  def __init__(self, beta):\n",
        "    super(FinalModel, self).__init__()\n",
        "    self.text_enc_model = BertMapping()\n",
        "    self.img_enc_model = TransformerMapping()\n",
        "    self.beta = beta\n",
        "\n",
        "  def cal_coeff(self, img_prob, text_prob) :\n",
        "    logp = torch.log(img_prob)\n",
        "    logp2 = torch.log(text_prob)\n",
        "    one_p = 1 - img_prob  # Don't compute gradient here\n",
        "    one_p.detach()\n",
        "    one_p2 = 1 - text_prob # Don't compute gradient here\n",
        "    one_p2.detach()\n",
        "    # M = 2\n",
        "    # Beta = 0.5\n",
        "    coeff = torch.pow(one_p2, self.beta) * logp + torch.pow(one_p, self.beta) * logp2\n",
        "    return coeff\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, img) :\n",
        "    img_enc = self.img_enc_model(img)\n",
        "    text_enc = self.text_enc_model(input_ids, attention_mask)\n",
        "    coeff = self.cal_coeff(img_enc, text_enc)\n",
        "    return coeff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzduJZMY6mWw"
      },
      "source": [
        "## Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYro2CHN67BW"
      },
      "outputs": [],
      "source": [
        "weights = [0.0003429355281207133, 0.0014265335235378032]\n",
        "sample_weights = [weights[t.argmax().int()] for t in train_output_img]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQp6uxYA67BX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_input_text, train_mask, train_input_img,train_output_img)\n",
        "# train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=3617)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhw-X8MB67BX"
      },
      "outputs": [],
      "source": [
        "test_data = TensorDataset(test_input_text, test_mask, test_input_img, test_output_img)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ESEusP72Xr9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def eval_model(model, val_loader) :\n",
        "  model.eval()\n",
        "  with torch.no_grad() :\n",
        "    final_out = []\n",
        "    final_lab = []\n",
        "    loss_val = 0\n",
        "    for idx, (val_input, val_mask, val_img, val_label) in enumerate(val_loader):\n",
        "        val_input = val_input.cuda()\n",
        "        val_mask = val_mask.cuda()\n",
        "        val_img = val_img.cuda()\n",
        "        val_label = val_label.cuda()\n",
        "        coeff = model(val_input, val_mask, val_img)\n",
        "        loss = -1*torch.mean(torch.sum(coeff*val_label,dim=1))\n",
        "        loss_val+= float(loss)\n",
        "        output = torch.argmax(coeff, dim=1)\n",
        "        val_label = torch.argmax(val_label, dim=1)\n",
        "        output = output.cpu().detach().numpy()\n",
        "        val_label = val_label.cpu().detach().numpy()\n",
        "        final_out.extend(list(output))\n",
        "        final_lab.extend(list(val_label))\n",
        "\n",
        "        del val_input\n",
        "        del val_label\n",
        "        del output\n",
        "        del coeff\n",
        "        del loss\n",
        "        del val_mask\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "  return classification_report(final_lab, final_out, output_dict=True), loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUTi-eCA7la4"
      },
      "outputs": [],
      "source": [
        "def train(net, opti, train_loader, num_epochs, val_loader, beta, best_f1_val):\n",
        "  loss_train = []\n",
        "  loss_test = []\n",
        "  best_f1 = best_f1_val\n",
        "  best_epoch = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for it, (text, mask,img, labels) in enumerate(train_loader):\n",
        "        opti.zero_grad()  \n",
        "        text, mask, img, labels = text.cuda(), mask.cuda(), img.cuda(), labels.cuda()\n",
        "        coeff = net(text, mask, img)\n",
        "        mul_out = coeff*labels\n",
        "        sum_out = torch.sum(mul_out, dim=1)\n",
        "        loss = -1*torch.mean(sum_out)\n",
        "        loss_val += float(loss.data)\n",
        "        loss.backward()\n",
        "        opti.step()\n",
        "\n",
        "        if (it + 1) % 50 == 0:\n",
        "            print(\"Iteration {} of epoch {} complete. Loss : {} \".format(it+1, epoch+1, loss.item()))\n",
        "\n",
        "        del text\n",
        "        del mask\n",
        "        del img\n",
        "        del labels\n",
        "        del loss\n",
        "        del mul_out\n",
        "        del sum_out\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss:{:.4f}'.format(epoch+1, num_epochs, loss_val))\n",
        "    loss_train.append(loss_val)\n",
        "    if((epoch+1)%1==0) :\n",
        "      report, loss_t= eval_model(net, val_loader)\n",
        "      f1 = report['macro avg']['f1-score']\n",
        "      loss_test.append(loss_t)\n",
        "      print(\"loss_test\", loss_t, \"beta\", beta)\n",
        "      print(\"classification_report\")\n",
        "      print(report)\n",
        "      if (f1>best_f1) :\n",
        "            print(\"best_f1_changed from \" + str(best_f1) + \" to \" + str(f1))\n",
        "            best_f1 = f1\n",
        "            best_epoch = epoch\n",
        "    print(\"--------------------------------------------------------------\")\n",
        "  return loss_train, loss_test, best_epoch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for beta in [0.2] : #Add more beta values in list to fine tune\n",
        "  net = FinalModel(beta).to(device)\n",
        "  opti = optim.Adam(net.parameters(), lr = 1e-4)\n",
        "  loss_train, loss_test, be = train(net, opti, train_dataloader, 30, test_dataloader, beta, 0)\n",
        "  plt.figure()\n",
        "  plt.plot(loss_train, label=\"loss_train\")\n",
        "  plt.plot(loss_test, label=\"loss_test\")\n",
        "  plt.plot([be,be], [0, max(loss_train)])\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zuh_dfCADrSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Weibo_multiplicative_model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d768311b21e4ef5833f4df1199974be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849fcde145e5449c84c90d085dbb0a53",
            "placeholder": "​",
            "style": "IPY_MODEL_6deada4511504449aa3b3f616ea976ad",
            "value": " 110k/110k [00:00&lt;00:00, 495kB/s]"
          },
          "model_module_version": "1.5.0"
        },
        "48aa0edd2ebd4cf3a4b624db71d958c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "603ff023842f46faa08d96376f5e8aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f310c7ca12a94e7fb60fb144e77566bc",
            "max": 109540,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba0f496bbda14457bfb1b6883874fdb8",
            "value": 109540
          },
          "model_module_version": "1.5.0"
        },
        "6deada4511504449aa3b3f616ea976ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          },
          "model_module_version": "1.5.0"
        },
        "76aa08b2f663499d834a967000cb705d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_603ff023842f46faa08d96376f5e8aee",
              "IPY_MODEL_0d768311b21e4ef5833f4df1199974be"
            ],
            "layout": "IPY_MODEL_48aa0edd2ebd4cf3a4b624db71d958c3"
          },
          "model_module_version": "1.5.0"
        },
        "849fcde145e5449c84c90d085dbb0a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "ba0f496bbda14457bfb1b6883874fdb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          },
          "model_module_version": "1.5.0"
        },
        "f310c7ca12a94e7fb60fb144e77566bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}