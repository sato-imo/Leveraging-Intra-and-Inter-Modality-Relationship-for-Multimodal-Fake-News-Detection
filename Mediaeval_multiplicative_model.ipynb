{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mediaeval_multiplicative_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc39bd1ae91c43a4b6941608bc39a993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_90f9b20f5b3d42769c7925621aba0bc9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_629b67f430f84203b3560b8aa2d2d677",
              "IPY_MODEL_8422c54fb8844c40aab67e6a2e9f473b"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "90f9b20f5b3d42769c7925621aba0bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "629b67f430f84203b3560b8aa2d2d677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b89deb0e2cbe4cb6b8b32e28b908512b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d12e85df0f5142efa3363f2bb570d4a7"
          },
          "model_module_version": "1.5.0"
        },
        "8422c54fb8844c40aab67e6a2e9f473b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b315fc2789d34ff587f8f3d49471a73a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.59MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c33a66222974db599eb626434696759"
          },
          "model_module_version": "1.5.0"
        },
        "b89deb0e2cbe4cb6b8b32e28b908512b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "d12e85df0f5142efa3363f2bb570d4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "b315fc2789d34ff587f8f3d49471a73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "4c33a66222974db599eb626434696759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4l-czDp67Hf",
        "outputId": "61b16ca3-66ad-4485-b1c1-8d4e87c827e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K135pWn-7B52",
        "outputId": "27886682-5f0f-41dc-b4f2-67b5076fd3af"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\r\u001b[K     |▏                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 32.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 22.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 20.7MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 21.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 16.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71kB 17.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92kB 16.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102kB 17.5MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 17.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122kB 17.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133kB 17.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143kB 17.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153kB 17.5MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 17.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174kB 17.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184kB 17.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194kB 17.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204kB 17.5MB/s eta 0:00:01\r\u001b[K     |████                            | 215kB 17.5MB/s eta 0:00:01\r\u001b[K     |████                            | 225kB 17.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235kB 17.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 245kB 17.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256kB 17.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 276kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 307kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 327kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 337kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 368kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 389kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 399kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 430kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 440kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 460kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 491kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 522kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 583kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 614kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 645kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 675kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 706kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 737kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 768kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 788kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 798kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 819kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 849kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 860kB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 880kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 890kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 911kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 921kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 952kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 972kB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 983kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993kB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7MB 17.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 17.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 58.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=be8624daee6c8596bd55389f4c2834b01fccecfe6edb96c90b6a4a65a40e7670\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ma-1A9-7EAG"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import Parameter\n",
        "from transformers import BertModel\n",
        "from transformers import BertConfig\n",
        "from transformers import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0VM4U0H1xc4"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58UQa8JX7GfQ"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1KNiTUO7JHd"
      },
      "source": [
        "def get_df(file):\n",
        "    return pd.read_csv(file,sep = '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNmYeC2Y7Lhm"
      },
      "source": [
        "train_df = get_df('/content/drive/My Drive/fake_news_dataset/mediaeval2016/train_posts.txt')\n",
        "test_df = get_df('/content/drive/My Drive/fake_news_dataset/mediaeval2016/test_posts.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiWUo7pQ7oXV"
      },
      "source": [
        "## Image Embedding processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeZqvnO57WOd"
      },
      "source": [
        "y_train = train_df['label'].eq('real').astype(int)\n",
        "y_test = test_df['label'].eq('real').astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqmSPqJF7Z5K"
      },
      "source": [
        "train_img = train_df['image_id(s)']\n",
        "test_img = test_df['image_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOItB7SNNfvO"
      },
      "source": [
        "# Load feature file which is a dictionary with image_id as key and feature vector as value. The feature vector is created using bottom up attention. \n",
        "dict_feat = pickle.load(open(\"/content/drive/My Drive/multi-modal/mediaeval/airsplay/dict_feat.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nOXTCrxNgx_",
        "outputId": "74eabec4-657e-4ddf-9c31-de50a117165e"
      },
      "source": [
        "# Find images for which feature vector size has a mismatch (the total number of fragments are not 36).\n",
        "c=0\n",
        "shape_not_match = []\n",
        "for key in dict_feat :\n",
        "  if(dict_feat[key].shape[0]!=36) :\n",
        "    shape_not_match.append(key)\n",
        "    c+=1\n",
        "print(c)\n",
        "print(len(shape_not_match))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43\n",
            "43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CVPIWdp7AJ0",
        "outputId": "671feab6-3dd3-4d50-ee56-89eb18dc81db"
      },
      "source": [
        "# Keep only those images for which the feature vector has 36 fragments (for training set)\n",
        "fin_train_img = []\n",
        "c=0\n",
        "ids_train = []\n",
        "for i in range(len(train_img)) :\n",
        "  img = train_img[i]\n",
        "  main_image = \"\"\n",
        "  if(',' in img) :\n",
        "    main_images = img.split(',')\n",
        "    for img in main_images :\n",
        "      if (img in list(dict_feat.keys())) :\n",
        "        main_image = img\n",
        "        break \n",
        "  else :\n",
        "    main_image = img\n",
        "\n",
        "  if((main_image in list(dict_feat.keys())) and (main_image not in shape_not_match)) :\n",
        "    fin_train_img.append(main_image)\n",
        "    ids_train.append(i)\n",
        "    c+=1\n",
        "\n",
        "print(c)\n",
        "print(len(ids_train))\n",
        "print(len(fin_train_img))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11789\n",
            "11789\n",
            "11789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbxf3Q7bNr2p"
      },
      "source": [
        "# Load feature file which is a dictionary with image_id as key and feature vector as value. The feature vector is created using bottom up attention. \n",
        "dict_feat_test = pickle.load(open(\"/content/drive/My Drive/multi-modal/mediaeval/airsplay/dict_feat_test.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvCqSyqpN1rF",
        "outputId": "ba084503-1fd1-4cf0-bee7-34c5f7cb4351"
      },
      "source": [
        "# Find images for which feature vector size has a mismatch (the total number of fragments are not 36).\n",
        "c=0\n",
        "shape_not_match_test = []\n",
        "for key in dict_feat_test :\n",
        "  if(dict_feat_test[key].shape[0]!=36) :\n",
        "    shape_not_match_test.append(key)\n",
        "    c+=1\n",
        "print(c)\n",
        "print(len(shape_not_match_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPsmgR5aJplz",
        "outputId": "505a7e90-4335-4a1e-cde0-2a293fb948da"
      },
      "source": [
        "# Keep only those images for which the feature vector has 36 fragments (for test set).\n",
        "fin_test_img = []\n",
        "c=0\n",
        "ids_test = []\n",
        "for i in range(len(test_img)) :\n",
        "  img = test_img[i]\n",
        "  main_image = \"\"\n",
        "  if(',' in img) :\n",
        "    main_images = img.split(',')\n",
        "    for img in main_images :\n",
        "      if (img in list(dict_feat_test.keys())) :\n",
        "        main_image = img\n",
        "        break \n",
        "  else :\n",
        "    main_image = img\n",
        "\n",
        "  if((main_image in list(dict_feat_test.keys())) and (main_image not in shape_not_match_test)) :\n",
        "    fin_test_img.append(main_image)\n",
        "    ids_test.append(i)\n",
        "    c+=1\n",
        "\n",
        "print(c)\n",
        "print(len(ids_test))\n",
        "print(len(fin_test_img))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "794\n",
            "794\n",
            "794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccHkX0gn7sGT"
      },
      "source": [
        "# Create final training set tensor for image feature vector and labels\n",
        "train_input_img = torch.zeros((11789,36,2048))\n",
        "train_output_img = torch.zeros((11789,2))\n",
        "\n",
        "for i in range(len(fin_train_img)) :\n",
        "  main_image = fin_train_img[i]\n",
        "  index = ids_train[i]\n",
        "  if (main_image not in dict_feat) :\n",
        "      ar = pickle.load(open(\"/content/drive/My Drive/multi-modal/mediaeval/airsplay/feat_train/\"+main_image+\".pkl\",\"rb\"))\n",
        "      dict_feat[main_image] = ar\n",
        "  else :\n",
        "    ar = dict_feat[main_image]\n",
        "  train_input_img[i] = ar\n",
        "  if (y_train[index]==0) :\n",
        "    train_output_img[i][0] = 1\n",
        "  else :\n",
        "    train_output_img[i][1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxlEkX5N8HkF"
      },
      "source": [
        "# Create final test set tensor for image feature vector and labels\n",
        "test_input_img = torch.zeros((794,36,2048))\n",
        "test_output_img = torch.zeros((794,2))\n",
        "\n",
        "for i in range(len(fin_test_img)) :\n",
        "  main_image = fin_test_img[i]\n",
        "  index = ids_test[i]\n",
        "  test_input_img[i] = dict_feat_test[main_image]\n",
        "  if (y_test[index]==0) :\n",
        "    test_output_img[i][0] = 1\n",
        "  else :\n",
        "    test_output_img[i][1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTj6eFRi8KcY",
        "outputId": "5af2175e-6b0d-433e-8a0d-1028cf420c1a"
      },
      "source": [
        "print(len(train_input_img))\n",
        "print(len(train_output_img))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11789\n",
            "11789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ZqqneY8M6_"
      },
      "source": [
        "# Code for this cell is from https://github.com/yiling2018/saem/blob/master/bert.py\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class BERTLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BERTLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "class BERTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BERTSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTAttention, self).__init__()\n",
        "        self.self = BERTSelfAttention(config)\n",
        "        self.output = BERTSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BERTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = gelu\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BERTLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BERTLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BERTLayer, self).__init__()\n",
        "        self.attention = BERTAttention(config)\n",
        "        self.intermediate = BERTIntermediate(config)\n",
        "        self.output = BERTOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkkdch-P8tJj"
      },
      "source": [
        "class TransformerMapping(nn.Module):\n",
        "    \"\"\" Self-attention layer for image branch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformerMapping, self).__init__()\n",
        "        bert_config = BertConfig.from_json_file(\"t_cfg_mediaeval.json\")\n",
        "        self.layer = BERTLayer(bert_config)\n",
        "        self.mapping = nn.Linear(2048, 256)\n",
        "        self.cls_layer = nn.Linear(256,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mapping(x)\n",
        "        attention_mask = torch.ones(x.size(0), x.size(1))\n",
        "        if torch.cuda.is_available():\n",
        "            attention_mask = attention_mask.cuda()\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.float()\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        hidden_states = self.layer(x, extended_attention_mask)\n",
        "        embed = torch.mean(hidden_states, 1)\n",
        "        feats = F.normalize(embed, p=2, dim=1)  \n",
        "        codes = self.cls_layer(feats)\n",
        "        return codes, feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO_uFAnJ8xB8"
      },
      "source": [
        "## Text embedding processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24I7nIw-800x"
      },
      "source": [
        "x_train = train_df['post_text']\n",
        "x_test = test_df['post_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhUPaIV-Vpb"
      },
      "source": [
        "# Get text only for those ids for which we have valid image feature vectors\n",
        "x_train_text = []\n",
        "x_test_text = []\n",
        "\n",
        "for ind in ids_train :\n",
        "  x_train_text.append(x_train[ind])\n",
        "\n",
        "for ind in ids_test :\n",
        "  x_test_text.append(x_test[ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab9yOLYw-lGs",
        "outputId": "00ffe617-532e-4433-fbed-c7b242ff8a68"
      },
      "source": [
        "print(len(x_train_text))\n",
        "print(len(x_test_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11789\n",
            "794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dc39bd1ae91c43a4b6941608bc39a993",
            "90f9b20f5b3d42769c7925621aba0bc9",
            "629b67f430f84203b3560b8aa2d2d677",
            "8422c54fb8844c40aab67e6a2e9f473b",
            "b89deb0e2cbe4cb6b8b32e28b908512b",
            "d12e85df0f5142efa3363f2bb570d4a7",
            "b315fc2789d34ff587f8f3d49471a73a",
            "4c33a66222974db599eb626434696759"
          ]
        },
        "id": "u-q9N_QFP5oi",
        "outputId": "7d126c7b-778e-42d5-c057-5fc96d5e574a"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc39bd1ae91c43a4b6941608bc39a993",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqBcad_A-rUp"
      },
      "source": [
        "def get_token_ids(x_train, x_test):\n",
        "    \n",
        "    token_tr = []\n",
        "    token_tst = []\n",
        "    count = 0\n",
        "    for sent in x_train :\n",
        "        tokens = tokenizer.encode(sent, add_special_tokens = True, max_length=512)\n",
        "        token_tr.append(tokens)\n",
        "        count+=1\n",
        "        if(count%1000==0):\n",
        "            print(count)\n",
        "    \n",
        "    for sent1 in x_test :\n",
        "        tokens1 = tokenizer.encode(sent1, add_special_tokens = True, max_length=512)\n",
        "        token_tst.append(tokens1)\n",
        "        count+=1\n",
        "        if(count%1000==0):\n",
        "            print(count)\n",
        "            \n",
        "    return token_tr, token_tst "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoJXks8v-u_x",
        "outputId": "59cacf45-2d6f-4865-a3e7-70b7a5915914"
      },
      "source": [
        "xtr_token, xtst_token = get_token_ids(x_train_text, x_test_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4b5Q5vO-0Vv"
      },
      "source": [
        "xtr_token = pad_sequences(xtr_token, maxlen=512, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "xtst_token = pad_sequences(xtst_token, maxlen=512, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubE3ykV2-2-g"
      },
      "source": [
        "attention_mask_tr = []\n",
        "attention_mask_tst = []\n",
        "for sent in xtr_token:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_mask_tr.append(att_mask)\n",
        "\n",
        "for sent in xtst_token:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_mask_tst.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5khuRUni-56K"
      },
      "source": [
        "train_input_text = torch.tensor(xtr_token)\n",
        "test_input_text = torch.tensor(xtst_token)\n",
        "\n",
        "train_mask = torch.tensor(attention_mask_tr)\n",
        "test_mask = torch.tensor(attention_mask_tst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mwnWu6_Jqf"
      },
      "source": [
        "def freeze_layers(model):\n",
        "    for child in model.children():\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeiIbEVM_KOQ"
      },
      "source": [
        "class BertMapping(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BertMapping, self).__init__()\n",
        "        bert_config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "        self.bert = BertModel(bert_config)\n",
        "        freeze_layers(self.bert)\n",
        "        final_dims = 256\n",
        "        Ks = [1, 2, 3]\n",
        "        in_channel = 1\n",
        "        out_channel = 512\n",
        "        embedding_dim = bert_config.hidden_size\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(in_channel, out_channel, (K, embedding_dim)) for K in Ks])\n",
        "        self.dropout = nn.Dropout(bert_config.hidden_dropout_prob)\n",
        "        self.mapping = nn.Linear(len(Ks)*out_channel, final_dims)\n",
        "        self.cls_layer = nn.Linear(final_dims, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids,attention_mask=attention_mask, return_dict=True)\n",
        "        x = outputs.last_hidden_state.unsqueeze(1)  # (batch_size, 1, token_num, embedding_dim)\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(batch_size, out_channel, W), ...]*len(Ks)\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "        output = torch.cat(x, 1)\n",
        "        output = self.dropout(output)\n",
        "        code = self.mapping(output)\n",
        "        # code = F.tanh(code)\n",
        "        feats = F.normalize(code, p=2, dim=1)\n",
        "        code = self.cls_layer(feats)\n",
        "        code = F.softmax(code, dim=1)\n",
        "        return code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rW5-KXPAbds"
      },
      "source": [
        "## Final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZheuYN1AfNh"
      },
      "source": [
        "class FinalModel(nn.Module) :\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  def __init__(self, beta):\n",
        "    super(FinalModel, self).__init__()\n",
        "    self.text_enc_model = BertMapping()\n",
        "    self.img_enc_model = TransformerMapping()\n",
        "    self.beta = beta\n",
        "    img_dims = 256\n",
        "    text_dims = 256\n",
        "\n",
        "  def cal_coeff(self, img_prob, text_prob) :\n",
        "    logp = torch.log(img_prob)\n",
        "    logp2 = torch.log(text_prob)\n",
        "    one_p = 1 - img_prob  # Don't compute gradient here\n",
        "    one_p.detach()\n",
        "    one_p2 = 1 - text_prob # Don't compute gradient here\n",
        "    one_p2.detach()\n",
        "    # M = 2\n",
        "    # Beta = 1\n",
        "    coeff = torch.pow(one_p2,self.beta) * logp + torch.pow(one_p,self.beta) * logp2\n",
        "    return coeff\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, img) :\n",
        "    img_enc = self.img_enc_model(img)\n",
        "    text_enc = self.text_enc_model(input_ids, attention_mask)\n",
        "    coeff = self.cal_coeff(img_enc, text_enc)\n",
        "    return coeff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs7-jpCm_VTe"
      },
      "source": [
        "## Training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s85hUn5o_QHv"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our test set.\n",
        "train_data = TensorDataset(train_input_text,train_mask, train_input_img, train_output_img)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvHVZ5SP_k6i"
      },
      "source": [
        "# Create the DataLoader for our validation set.\n",
        "test_data = TensorDataset(test_input_text,test_mask, test_input_img, test_output_img)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGyo4don_-6X"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def eval_model(model, val_loader) :\n",
        "  model.eval()\n",
        "  with torch.no_grad() :\n",
        "    final_out = []\n",
        "    final_lab = []\n",
        "    loss_val = 0\n",
        "    for idx, (val_input, val_mask, val_img, val_label) in enumerate(val_loader):\n",
        "      # try:\n",
        "        val_input = val_input.cuda()\n",
        "        val_mask = val_mask.cuda()\n",
        "        val_img = val_img.cuda()\n",
        "        val_label = val_label.cuda()\n",
        "        coeff = model(val_input, val_mask, val_img)\n",
        "        loss = -1*torch.mean(torch.sum(coeff*val_label,dim=1))\n",
        "        loss_val+= float(loss)\n",
        "        output = torch.argmax(coeff, dim=1)\n",
        "        val_label = torch.argmax(val_label, dim=1)\n",
        "        output = output.cpu().detach().numpy()\n",
        "        val_label = val_label.cpu().detach().numpy()\n",
        "        final_out.extend(list(output))\n",
        "        final_lab.extend(list(val_label))\n",
        "\n",
        "        del val_input\n",
        "        del val_label\n",
        "        del output\n",
        "        del coeff\n",
        "        del loss\n",
        "        del val_mask\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "  return classification_report(final_lab, final_out, output_dict=True), loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc09PM0-AHWN"
      },
      "source": [
        "def train(net, opti, train_loader, num_epochs, val_loader, beta, best_f1_val):\n",
        "  loss_train = []\n",
        "  loss_test = []\n",
        "  best_f1 = best_f1_val\n",
        "  best_epoch = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    loss_val = 0\n",
        "    for it, (text, mask,img, labels) in enumerate(train_loader):\n",
        "        torch.cuda.empty_cache()\n",
        "        opti.zero_grad()  \n",
        "        text, mask, img, labels = text.cuda(), mask.cuda(), img.cuda(), labels.cuda()\n",
        "        coeff = net(text, mask, img)\n",
        "        mul_out = coeff*labels\n",
        "        sum_out = torch.sum(mul_out, dim=1)\n",
        "        loss = -1*torch.mean(sum_out)\n",
        "        loss_val += float(loss.data)\n",
        "        loss.backward()\n",
        "        opti.step()\n",
        "\n",
        "        if (it + 1) % 100 == 0:\n",
        "            print(\"Iteration {} of epoch {} complete. Loss : {}\".format(it+1, epoch+1, loss.item()))\n",
        "\n",
        "        del text\n",
        "        del mask\n",
        "        del img\n",
        "        del labels\n",
        "        del loss\n",
        "        del mul_out\n",
        "        del sum_out\n",
        "        del coeff\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss:{:.4f}'.format(epoch+1, num_epochs, loss_val))\n",
        "    loss_train.append(loss_val)\n",
        "    if((epoch+1)%1==0) :\n",
        "      report, loss_t= eval_model(net, val_loader)\n",
        "      f1 = report['macro avg']['f1-score']\n",
        "      loss_test.append(loss_t)\n",
        "      print(\"loss_test\", loss_t, \"beta\", beta)\n",
        "      print(\"classification_report\")\n",
        "      print(report)\n",
        "      if (f1>best_f1) :\n",
        "            print(\"best_f1_changed from \" + str(best_f1) + \" to \" + str(f1))\n",
        "            best_f1 = f1\n",
        "            best_epoch = epoch\n",
        "      print(\"--------------------------------------------------------------\")\n",
        "  return loss_train, loss_test, best_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for beta in [0.5] : #Add more beta values in list to fine tune\n",
        "  torch.cuda.empty_cache()\n",
        "  net = FinalModel(beta).to(device)\n",
        "  opti = optim.Adam(net.parameters(), lr = 1e-4)\n",
        "  loss_train, loss_test, be = train(net, opti, train_dataloader, 15, test_dataloader, beta)\n",
        "  plt.figure()\n",
        "  plt.plot(loss_train, label=\"loss_train\")\n",
        "  plt.plot(loss_test, label=\"loss_test\")\n",
        "  plt.plot([be,be], [0, max(loss_train)])\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TGyV8faM7mje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}